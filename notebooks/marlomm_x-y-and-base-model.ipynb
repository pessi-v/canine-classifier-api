{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":308849,"sourceType":"datasetVersion","datasetId":129000}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from tensorflow.keras.utils import image_dataset_from_directory\n\nimages = image_dataset_from_directory(\"/kaggle/input/stanford-dogs-dataset-traintest/cropped/cropped/train\", \n                                      seed=1234, subset=\"both\", validation_split=0.2, batch_size=32, \n                                      labels=\"inferred\", label_mode = \"categorical\")\nimages[0]","metadata":{"execution":{"iopub.status.busy":"2024-02-06T11:11:40.854021Z","iopub.execute_input":"2024-02-06T11:11:40.854270Z","iopub.status.idle":"2024-02-06T11:12:00.067439Z","shell.execute_reply.started":"2024-02-06T11:11:40.854247Z","shell.execute_reply":"2024-02-06T11:12:00.066466Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-02-06 11:11:43.038786: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-06 11:11:43.038899: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-06 11:11:43.258628: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Found 12000 files belonging to 120 classes.\nUsing 9600 files for training.\nUsing 2400 files for validation.\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 120), dtype=tf.float32, name=None))>"},"metadata":{}}]},{"cell_type":"code","source":"from tensorflow.keras.applications.vgg16 import VGG16\n\ndef load_model():\n\n    model = VGG16(include_top=False, weights='imagenet', input_shape=(256, 256, 3))\n\n    return model\n\ndef set_nontrainable_layers(model):\n\n    model.trainable = False\n\n    return model\n\nmodel = load_model()\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-02-06T11:12:00.069078Z","iopub.execute_input":"2024-02-06T11:12:00.069415Z","iopub.status.idle":"2024-02-06T11:12:01.005453Z","shell.execute_reply.started":"2024-02-06T11:12:00.069388Z","shell.execute_reply":"2024-02-06T11:12:01.004514Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n58889256/58889256 [==============================] - 0s 0us/step\nModel: \"vgg16\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 256, 256, 3)]     0         \n                                                                 \n block1_conv1 (Conv2D)       (None, 256, 256, 64)      1792      \n                                                                 \n block1_conv2 (Conv2D)       (None, 256, 256, 64)      36928     \n                                                                 \n block1_pool (MaxPooling2D)  (None, 128, 128, 64)      0         \n                                                                 \n block2_conv1 (Conv2D)       (None, 128, 128, 128)     73856     \n                                                                 \n block2_conv2 (Conv2D)       (None, 128, 128, 128)     147584    \n                                                                 \n block2_pool (MaxPooling2D)  (None, 64, 64, 128)       0         \n                                                                 \n block3_conv1 (Conv2D)       (None, 64, 64, 256)       295168    \n                                                                 \n block3_conv2 (Conv2D)       (None, 64, 64, 256)       590080    \n                                                                 \n block3_conv3 (Conv2D)       (None, 64, 64, 256)       590080    \n                                                                 \n block3_pool (MaxPooling2D)  (None, 32, 32, 256)       0         \n                                                                 \n block4_conv1 (Conv2D)       (None, 32, 32, 512)       1180160   \n                                                                 \n block4_conv2 (Conv2D)       (None, 32, 32, 512)       2359808   \n                                                                 \n block4_conv3 (Conv2D)       (None, 32, 32, 512)       2359808   \n                                                                 \n block4_pool (MaxPooling2D)  (None, 16, 16, 512)       0         \n                                                                 \n block5_conv1 (Conv2D)       (None, 16, 16, 512)       2359808   \n                                                                 \n block5_conv2 (Conv2D)       (None, 16, 16, 512)       2359808   \n                                                                 \n block5_conv3 (Conv2D)       (None, 16, 16, 512)       2359808   \n                                                                 \n block5_pool (MaxPooling2D)  (None, 8, 8, 512)         0         \n                                                                 \n=================================================================\nTotal params: 14714688 (56.13 MB)\nTrainable params: 14714688 (56.13 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras import layers, models\nimport tensorflow as tf\n\ndef add_last_layers(model):\n    '''Take a pre-trained model, set its parameters as non-trainable, and add additional trainable layers on top'''\n    flattening_layer = layers.Flatten()\n    dense_layer = layers.Dense(500, activation='relu')\n    prediction_layer = layers.Dense(120, activation='softmax')\n    model = tf.keras.Sequential([\n      model,\n      flattening_layer,\n      dense_layer,\n      prediction_layer\n    ])\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-02-06T11:12:01.006725Z","iopub.execute_input":"2024-02-06T11:12:01.007095Z","iopub.status.idle":"2024-02-06T11:12:01.015785Z","shell.execute_reply.started":"2024-02-06T11:12:01.007061Z","shell.execute_reply":"2024-02-06T11:12:01.014464Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import optimizers\n\ndef build_model():\n    base_model = load_model()\n    base_model = set_nontrainable_layers(base_model)\n    model = add_last_layers(base_model)\n    model.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.Adam(learning_rate=1e-4),\n              metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-02-06T11:12:01.017971Z","iopub.execute_input":"2024-02-06T11:12:01.018418Z","iopub.status.idle":"2024-02-06T11:12:01.027962Z","shell.execute_reply.started":"2024-02-06T11:12:01.018390Z","shell.execute_reply":"2024-02-06T11:12:01.026899Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications.vgg16 import preprocess_input\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nimages_preprocessed = images[0].map(\n  lambda x, y: (preprocess_input(x), y))\nimages_val_prepocessed = images[1].map(\n  lambda x, y: (preprocess_input(x), y))\n\nmodel = build_model()\nmodel.summary()\nes = EarlyStopping(patience=3)\nhistory = model.fit(images_preprocessed,\n          epochs=100,\n          batch_size=32,\n          callbacks=es,\n          validation_data=images_val_preprocessed,\n          verbose=1)","metadata":{"execution":{"iopub.status.busy":"2024-02-06T11:12:01.029151Z","iopub.execute_input":"2024-02-06T11:12:01.029472Z","iopub.status.idle":"2024-02-06T11:18:35.255047Z","shell.execute_reply.started":"2024-02-06T11:12:01.029447Z","shell.execute_reply":"2024-02-06T11:18:35.254120Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n vgg16 (Functional)          (None, 8, 8, 512)         14714688  \n                                                                 \n flatten (Flatten)           (None, 32768)             0         \n                                                                 \n dense (Dense)               (None, 500)               16384500  \n                                                                 \n dense_1 (Dense)             (None, 120)               60120     \n                                                                 \n=================================================================\nTotal params: 31159308 (118.86 MB)\nTrainable params: 16444620 (62.73 MB)\nNon-trainable params: 14714688 (56.13 MB)\n_________________________________________________________________\nEpoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1707217934.671841     111 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"300/300 [==============================] - 72s 197ms/step - loss: 6.2604 - accuracy: 0.1120 - val_loss: 3.7971 - val_accuracy: 0.1950\nEpoch 2/100\n300/300 [==============================] - 64s 213ms/step - loss: 1.7720 - accuracy: 0.5498 - val_loss: 3.1341 - val_accuracy: 0.3579\nEpoch 3/100\n300/300 [==============================] - 64s 211ms/step - loss: 0.3929 - accuracy: 0.8878 - val_loss: 3.1057 - val_accuracy: 0.4108\nEpoch 4/100\n300/300 [==============================] - 64s 212ms/step - loss: 0.0886 - accuracy: 0.9782 - val_loss: 3.1483 - val_accuracy: 0.4421\nEpoch 5/100\n300/300 [==============================] - 65s 214ms/step - loss: 0.0410 - accuracy: 0.9939 - val_loss: 3.2470 - val_accuracy: 0.4454\nEpoch 6/100\n300/300 [==============================] - 64s 213ms/step - loss: 0.0428 - accuracy: 0.9928 - val_loss: 3.3234 - val_accuracy: 0.4617\n","output_type":"stream"}]},{"cell_type":"code","source":"test_images = image_dataset_from_directory(\"/kaggle/input/stanford-dogs-dataset-traintest/cropped/cropped/test\", \n                                      seed=1234, batch_size=32, \n                                      labels=\"inferred\", label_mode = \"categorical\")\nresults = model.evaluate(test_images)","metadata":{"execution":{"iopub.status.busy":"2024-02-06T11:18:35.256320Z","iopub.execute_input":"2024-02-06T11:18:35.256649Z","iopub.status.idle":"2024-02-06T11:19:38.867309Z","shell.execute_reply.started":"2024-02-06T11:18:35.256620Z","shell.execute_reply":"2024-02-06T11:19:38.866366Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Found 12000 files belonging to 120 classes.\n375/375 [==============================] - 63s 167ms/step - loss: 4.5205 - accuracy: 0.3626\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"[4.520523548126221, 0.36258333921432495]"},"metadata":{}}]},{"cell_type":"markdown","source":"# Why not iterate over different base models to check which one is the best?","metadata":{}},{"cell_type":"code","source":"import tensorflow.keras.applications as pmodels\nfrom tensorflow.keras import layers, models, optimizers\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.utils import image_dataset_from_directory\n\ndef test_base_models():\n    images = image_dataset_from_directory(\"/kaggle/input/stanford-dogs-dataset-traintest/cropped/cropped/train\", \n                                      seed=1234, subset=\"both\", validation_split=0.2, batch_size=32, \n                                      labels=\"inferred\", label_mode = \"categorical\")\n    test_images = image_dataset_from_directory(\"/kaggle/input/stanford-dogs-dataset-traintest/cropped/cropped/test\", \n                                      seed=1234, batch_size=32, \n                                      labels=\"inferred\", label_mode = \"categorical\")\n    base_models = [pmodels.vgg16.VGG16, pmodels.efficientnet_v2.EfficientNetV2L, pmodels.inception_v3.InceptionV3, pmodels.xception.Xception, pmodels.MobileNetV3Large]\n    preprocess_func = [pmodels.vgg16.preprocess_input, do_nothing, pmodels.inception_v3.preprocess_input, pmodels.xception.preprocess_input, do_nothing]\n    histories = []\n    scores = []\n    models = []\n    for index, base_model in enumerate(base_models):\n        model = build_model(base_model)\n        images_train = images[0].map(lambda x, y: (preprocess_func[index](x), y))\n        images_val = images[1].map(lambda x, y: (preprocess_func[index](x), y))\n        images_test = test_images.map(lambda x, y: (preprocess_func[index](x), y))\n        history = train_model(model, images_train, images_val)\n        score = evaluate_model(model, images_test)\n        histories.append(history)\n        scores.append(score)\n        models.append(model)\n    return histories, scores, models, base_models\n\ndef add_last_layers(model):\n    '''Take a pre-trained model, set its parameters as non-trainable, and add additional trainable layers on top'''\n    flattening_layer = layers.Flatten()\n    dense_layer = layers.Dense(500, activation='relu')\n    prediction_layer = layers.Dense(120, activation='softmax')\n    model = models.Sequential([\n      model,\n      flattening_layer,\n      dense_layer,\n      prediction_layer\n    ])\n\n    return model\n\ndef build_model(base_model):\n    model = load_model(base_model)\n    model = set_nontrainable_layers(model)\n    model = add_last_layers(model)\n    model.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.Adam(),\n              metrics=['accuracy'])\n    return model\n\ndef load_model(base_model):\n    model = base_model(include_top=False, weights='imagenet', input_shape=(256, 256, 3))\n    return model\n\ndef set_nontrainable_layers(model):\n    model.trainable = False\n    return model\n\ndef train_model(model, images_train, images_val):\n    model.summary()\n    es = EarlyStopping(monitor=\"val_accuracy\", patience=3, restore_best_weights=True)\n    history = model.fit(images_train,\n              epochs=100,\n              batch_size=32,\n              callbacks=es,\n              validation_data=images_val,\n              verbose=1)\n    return history\n\ndef evaluate_model(model, images_test):\n    return model.evaluate(images_test)\n\ndef do_nothing(object):\n    return object\n\nhistories, scores, models, base_models = test_base_models()","metadata":{"execution":{"iopub.status.busy":"2024-02-06T14:12:28.840158Z","iopub.execute_input":"2024-02-06T14:12:28.840483Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"2024-02-06 14:12:30.939147: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-06 14:12:30.939232: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-06 14:12:31.113606: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Found 12000 files belonging to 120 classes.\nUsing 9600 files for training.\nUsing 2400 files for validation.\nFound 8580 files belonging to 120 classes.\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n58889256/58889256 [==============================] - 0s 0us/step\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n vgg16 (Functional)          (None, 8, 8, 512)         14714688  \n                                                                 \n flatten (Flatten)           (None, 32768)             0         \n                                                                 \n dense (Dense)               (None, 500)               16384500  \n                                                                 \n dense_1 (Dense)             (None, 120)               60120     \n                                                                 \n=================================================================\nTotal params: 31159308 (118.86 MB)\nTrainable params: 16444620 (62.73 MB)\nNon-trainable params: 14714688 (56.13 MB)\n_________________________________________________________________\nEpoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1707228782.952217      91 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"300/300 [==============================] - 71s 194ms/step - loss: 6.3200 - accuracy: 0.1098 - val_loss: 3.7203 - val_accuracy: 0.2033\nEpoch 2/100\n300/300 [==============================] - 64s 211ms/step - loss: 1.7452 - accuracy: 0.5562 - val_loss: 3.2862 - val_accuracy: 0.3296\nEpoch 3/100\n300/300 [==============================] - 63s 210ms/step - loss: 0.4148 - accuracy: 0.8838 - val_loss: 3.2572 - val_accuracy: 0.3958\nEpoch 4/100\n300/300 [==============================] - 64s 212ms/step - loss: 0.1054 - accuracy: 0.9766 - val_loss: 3.2691 - val_accuracy: 0.4267\nEpoch 5/100\n300/300 [==============================] - 64s 211ms/step - loss: 0.0575 - accuracy: 0.9910 - val_loss: 3.3641 - val_accuracy: 0.4529\nEpoch 6/100\n300/300 [==============================] - 64s 211ms/step - loss: 0.0525 - accuracy: 0.9911 - val_loss: 3.4077 - val_accuracy: 0.4442\nEpoch 7/100\n300/300 [==============================] - 64s 212ms/step - loss: 0.0334 - accuracy: 0.9969 - val_loss: 3.3992 - val_accuracy: 0.4600\nEpoch 8/100\n300/300 [==============================] - 64s 211ms/step - loss: 0.0520 - accuracy: 0.9886 - val_loss: 3.8315 - val_accuracy: 0.4304\nEpoch 9/100\n300/300 [==============================] - 64s 211ms/step - loss: 0.3298 - accuracy: 0.9221 - val_loss: 4.6670 - val_accuracy: 0.3967\n269/269 [==============================] - 47s 173ms/step - loss: 3.3105 - accuracy: 0.4719\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-l_notop.h5\n473176280/473176280 [==============================] - 2s 0us/step\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n efficientnetv2-l (Function  (None, 8, 8, 1280)        117746848 \n al)                                                             \n                                                                 \n flatten_1 (Flatten)         (None, 81920)             0         \n                                                                 \n dense_2 (Dense)             (None, 500)               40960500  \n                                                                 \n dense_3 (Dense)             (None, 120)               60120     \n                                                                 \n=================================================================\nTotal params: 158767468 (605.65 MB)\nTrainable params: 41020620 (156.48 MB)\nNon-trainable params: 117746848 (449.17 MB)\n_________________________________________________________________\nEpoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"2024-02-06 14:24:00.707732: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_1/efficientnetv2-l/block1b_drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"300/300 [==============================] - 228s 656ms/step - loss: 0.9261 - accuracy: 0.8021 - val_loss: 0.4908 - val_accuracy: 0.8879\nEpoch 2/100\n300/300 [==============================] - 186s 620ms/step - loss: 0.2822 - accuracy: 0.9277 - val_loss: 0.4540 - val_accuracy: 0.9025\nEpoch 3/100\n291/300 [============================>.] - ETA: 4s - loss: 0.1859 - accuracy: 0.9460","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\naccuracies = [i[1] for i in scores]\nmodel_names = [i.__name__ for i in base_models]\nplt.bar(model_names, height=accuracies)","metadata":{"execution":{"iopub.status.busy":"2024-02-06T13:06:25.319779Z","iopub.execute_input":"2024-02-06T13:06:25.320638Z","iopub.status.idle":"2024-02-06T13:06:25.461894Z","shell.execute_reply.started":"2024-02-06T13:06:25.320600Z","shell.execute_reply":"2024-02-06T13:06:25.461087Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"<BarContainer object of 4 artists>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]}]}